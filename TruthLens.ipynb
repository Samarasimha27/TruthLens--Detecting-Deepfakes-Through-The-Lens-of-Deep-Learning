{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ec4d2f-4677-4cc3-805d-0981568d6e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Resumed from epoch 4.\n",
      "Epoch 5/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [44:39<00:00,  6.68s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0876 Acc: 0.9267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:12<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1000 Acc: 0.9300\n",
      "Checkpoint saved at epoch 5.\n",
      "Epoch 6/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [40:09<00:00,  6.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0696 Acc: 0.9327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:11<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1150 Acc: 0.9220\n",
      "Epoch 7/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [40:13<00:00,  6.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0644 Acc: 0.9354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:11<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1674 Acc: 0.9075\n",
      "Epoch 8/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 5/401 [00:26<35:37,  5.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 227\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensemble_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    229\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Testing phase\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, dataloader, criterion):\n",
      "Cell \u001b[1;32mIn[1], line 174\u001b[0m, in \u001b[0;36mtrain_model_with_early_stopping\u001b[1;34m(model, criterion, optimizer, num_epochs, patience, checkpoint_path, resume)\u001b[0m\n\u001b[0;32m    171\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    172\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 174\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m, in \u001b[0;36mFaceCroppingDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     35\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Crop the first detected face\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     x, y, w, h \u001b[38;5;241m=\u001b[39m (faces[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mleft(), faces[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtop(), faces[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mwidth(), faces[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mheight())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Define a custom dataset class to include face detection and cropping\n",
    "class FaceCroppingDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(FaceCroppingDataset, self).__init__(root, transform)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "\n",
    "        # Convert BGR (OpenCV) to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = detector(image, 1)\n",
    "        if len(faces) > 0:\n",
    "            # Crop the first detected face\n",
    "            x, y, w, h = (faces[0].left(), faces[0].top(), faces[0].width(), faces[0].height())\n",
    "            image = image[y:y+h, x:x+w]\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        # Convert image to PIL format and apply transformations\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Data Preparation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Train', transform=data_transforms['train'])\n",
    "val_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Validation', transform=data_transforms['val'])\n",
    "test_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Test', transform=data_transforms['test'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[0] is not None and b[1] is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load pre-trained models\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Unfreeze the top layers for fine-tuning\n",
    "for model in [resnet, efficientnet, mobilenet]:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name or \"blocks\" in name or \"features\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Define Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ensemble_model.parameters()), lr=0.0001)\n",
    "\n",
    "# Save checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, best_acc, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "# Train function with early stopping\n",
    "def train_model_with_early_stopping(\n",
    "    model, criterion, optimizer, num_epochs=20, patience=3, checkpoint_path=\"best_model.pth\", resume=False\n",
    "):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_loss = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resumed from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch...\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = running_loss / len(val_dataset)\n",
    "        val_acc = running_corrects.double() / len(val_dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_checkpoint(epoch, model, optimizer, val_acc, checkpoint_path)\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model_with_early_stopping(\n",
    "    ensemble_model, criterion, optimizer, num_epochs=20, patience=3, checkpoint_path=\"new_model.pth\", resume=True\n",
    ")\n",
    "\n",
    "# Testing phase\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    loss = running_loss / len(dataloader.dataset)\n",
    "    acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    print(f'Test Loss: {loss:.4f} Acc: {acc:.4f}')\n",
    "    print(f'Precision: {precision:.4f} Recall: {recall:.4f} F1 Score: {f1:.4f}')\n",
    "    print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "    return loss, acc, precision, recall, f1, roc_auc\n",
    "\n",
    "test_loss, test_acc, test_precision, test_recall, test_f1, test_roc_auc = evaluate_model(trained_model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9afb2c-b036-4b56-b8a8-1d1974018c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Resumed from epoch 5.\n",
      "Epoch 6/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:52<00:00,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0696 Acc: 0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:31<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1149 Acc: 0.9270\n",
      "Checkpoint saved at epoch 6.\n",
      "Epoch 7/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:36<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0709 Acc: 0.9322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:09<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1146 Acc: 0.9185\n",
      "Checkpoint saved at epoch 7.\n",
      "Epoch 8/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:29<00:00,  6.21s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0586 Acc: 0.9385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:16<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1208 Acc: 0.9275\n",
      "Epoch 9/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:19<00:00,  6.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0505 Acc: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:19<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1123 Acc: 0.9345\n",
      "Checkpoint saved at epoch 9.\n",
      "Epoch 10/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [40:23<00:00,  6.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0443 Acc: 0.9415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [03:21<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1239 Acc: 0.9255\n",
      "Epoch 11/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [46:22<00:00,  6.94s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0427 Acc: 0.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:05<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1369 Acc: 0.9265\n",
      "Epoch 12/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [40:50<00:00,  6.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0391 Acc: 0.9436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:06<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1399 Acc: 0.9275\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Early stopping triggered after 12 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 63/63 [02:15<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9373\n",
      "Recall: 0.9917\n",
      "F1 Score: 0.9637\n",
      "ROC AUC: 0.9617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Define a custom dataset class to include face detection and cropping\n",
    "class FaceCroppingDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(FaceCroppingDataset, self).__init__(root, transform)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "\n",
    "        # Convert BGR (OpenCV) to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = detector(image, 1)\n",
    "        if len(faces) > 0:\n",
    "            # Crop the first detected face\n",
    "            x, y, w, h = (faces[0].left(), faces[0].top(), faces[0].width(), faces[0].height())\n",
    "            image = image[y:y+h, x:x+w]\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        # Convert image to PIL format and apply transformations\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Data Preparation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Train', transform=data_transforms['train'])\n",
    "val_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Validation', transform=data_transforms['val'])\n",
    "test_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Test', transform=data_transforms['test'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[0] is not None and b[1] is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load pre-trained models\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Unfreeze the top layers for fine-tuning\n",
    "for model in [resnet, efficientnet, mobilenet]:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name or \"blocks\" in name or \"features\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Define Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ensemble_model.parameters()), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Save checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, best_acc, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "# Train function with early stopping\n",
    "def train_model_with_early_stopping(\n",
    "    model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"best_model.pth\", resume=False\n",
    "):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_loss = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resumed from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch...\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = running_loss / len(val_dataset)\n",
    "        val_acc = running_corrects.double() / len(val_dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_checkpoint(epoch, model, optimizer, val_acc, checkpoint_path)\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "ensemble_model = train_model_with_early_stopping(\n",
    "    ensemble_model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"new_model.pth\", resume=True\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "ensemble_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = ensemble_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343d4fa-e963-4594-8b5a-e263c660b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Resumed from epoch 9.\n",
      "Epoch 10/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:56<00:00,  6.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0457 Acc: 0.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  81%|████████  | 51/63 [03:18<00:55,  4.64s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Define a custom dataset class to include face detection and cropping\n",
    "class FaceCroppingDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(FaceCroppingDataset, self).__init__(root, transform)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "\n",
    "        # Convert BGR (OpenCV) to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = detector(image, 1)\n",
    "        if len(faces) > 0:\n",
    "            # Crop the first detected face\n",
    "            x, y, w, h = (faces[0].left(), faces[0].top(), faces[0].width(), faces[0].height())\n",
    "            image = image[y:y+h, x:x+w]\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        # Convert image to PIL format and apply transformations\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Data Preparation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Train', transform=data_transforms['train'])\n",
    "val_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Validation', transform=data_transforms['val'])\n",
    "test_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Test', transform=data_transforms['test'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[0] is not None and b[1] is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load pre-trained models\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Unfreeze the top layers for fine-tuning\n",
    "for model in [resnet, efficientnet, mobilenet]:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name or \"blocks\" in name or \"features\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Define Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ensemble_model.parameters()), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Save checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, best_acc, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "# Train function with early stopping\n",
    "def train_model_with_early_stopping(\n",
    "    model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"best_model.pth\", resume=False\n",
    "):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_loss = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resumed from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch...\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = running_loss / len(val_dataset)\n",
    "        val_acc = running_corrects.double() / len(val_dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_checkpoint(epoch, model, optimizer, val_acc, checkpoint_path)\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "ensemble_model = train_model_with_early_stopping(\n",
    "    ensemble_model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"new_model.pth\", resume=True\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "ensemble_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = ensemble_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b93e2c7-14fc-4193-b95c-9c7e3d313093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Resumed from epoch 10.\n",
      "Epoch 11/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [40:47<00:00,  6.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0406 Acc: 0.9428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:19<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1359 Acc: 0.9335\n",
      "Checkpoint saved at epoch 11.\n",
      "Epoch 12/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [46:56<00:00,  7.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0411 Acc: 0.9427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [04:27<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1231 Acc: 0.9320\n",
      "Checkpoint saved at epoch 12.\n",
      "Epoch 13/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 35/401 [04:44<49:31,  8.12s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 235\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping and learning rate scheduler\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m ensemble_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensemble_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    237\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m    240\u001b[0m ensemble_model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[1], line 179\u001b[0m, in \u001b[0;36mtrain_model_with_early_stopping\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs, patience, checkpoint_path, resume)\u001b[0m\n\u001b[0;32m    176\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    177\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 179\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m, in \u001b[0;36mFaceCroppingDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     35\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Crop the first detected face\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     x, y, w, h \u001b[38;5;241m=\u001b[39m (faces[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mleft(), faces[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtop(), faces[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mwidth(), faces[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mheight())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Define a custom dataset class to include face detection and cropping\n",
    "class FaceCroppingDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(FaceCroppingDataset, self).__init__(root, transform)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "\n",
    "        # Convert BGR (OpenCV) to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = detector(image, 1)\n",
    "        if len(faces) > 0:\n",
    "            # Crop the first detected face\n",
    "            x, y, w, h = (faces[0].left(), faces[0].top(), faces[0].width(), faces[0].height())\n",
    "            image = image[y:y+h, x:x+w]\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        # Convert image to PIL format and apply transformations\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Data Preparation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Train', transform=data_transforms['train'])\n",
    "val_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Validation', transform=data_transforms['val'])\n",
    "test_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Test', transform=data_transforms['test'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[0] is not None and b[1] is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load pre-trained models\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Unfreeze the top layers for fine-tuning\n",
    "for model in [resnet, efficientnet, mobilenet]:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name or \"blocks\" in name or \"features\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Define Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ensemble_model.parameters()), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Save checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, best_acc, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "# Train function with early stopping\n",
    "def train_model_with_early_stopping(\n",
    "    model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"best_model.pth\", resume=False\n",
    "):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_loss = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resumed from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch...\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = running_loss / len(val_dataset)\n",
    "        val_acc = running_corrects.double() / len(val_dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_checkpoint(epoch, model, optimizer, val_acc, checkpoint_path)\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "ensemble_model = train_model_with_early_stopping(\n",
    "    ensemble_model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"new_model.pth\", resume=True\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "ensemble_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = ensemble_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1994a87e-97f3-4837-9b3d-cf990334e9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Resumed from epoch 12.\n",
      "Epoch 13/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:34<00:00,  6.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0379 Acc: 0.9446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:26<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1833 Acc: 0.9215\n",
      "Checkpoint saved at epoch 13.\n",
      "Epoch 14/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:35<00:00,  6.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0293 Acc: 0.9482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:26<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1874 Acc: 0.9145\n",
      "Epoch 15/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:27<00:00,  6.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0349 Acc: 0.9468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:26<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1980 Acc: 0.9225\n",
      "Epoch 16/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:23<00:00,  6.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0273 Acc: 0.9494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:27<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1355 Acc: 0.9300\n",
      "Checkpoint saved at epoch 16.\n",
      "Epoch 17/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [41:50<00:00,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0278 Acc: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:40<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1556 Acc: 0.9270\n",
      "Epoch 18/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/401 [00:25<57:23,  8.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 235\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping and learning rate scheduler\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m ensemble_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensemble_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    237\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m    240\u001b[0m ensemble_model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[1], line 185\u001b[0m, in \u001b[0;36mtrain_model_with_early_stopping\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs, patience, checkpoint_path, resume)\u001b[0m\n\u001b[0;32m    183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    184\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m--> 185\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    188\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Define a custom dataset class to include face detection and cropping\n",
    "class FaceCroppingDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(FaceCroppingDataset, self).__init__(root, transform)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "\n",
    "        # Convert BGR (OpenCV) to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = detector(image, 1)\n",
    "        if len(faces) > 0:\n",
    "            # Crop the first detected face\n",
    "            x, y, w, h = (faces[0].left(), faces[0].top(), faces[0].width(), faces[0].height())\n",
    "            image = image[y:y+h, x:x+w]\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        # Convert image to PIL format and apply transformations\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Data Preparation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Train', transform=data_transforms['train'])\n",
    "val_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Validation', transform=data_transforms['val'])\n",
    "test_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Test', transform=data_transforms['test'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[0] is not None and b[1] is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load pre-trained models\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Unfreeze the top layers for fine-tuning\n",
    "for model in [resnet, efficientnet, mobilenet]:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name or \"blocks\" in name or \"features\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Define Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ensemble_model.parameters()), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Save checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, best_acc, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "# Train function with early stopping\n",
    "def train_model_with_early_stopping(\n",
    "    model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"best_model.pth\", resume=False\n",
    "):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_loss = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resumed from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch...\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = running_loss / len(val_dataset)\n",
    "        val_acc = running_corrects.double() / len(val_dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_checkpoint(epoch, model, optimizer, val_acc, checkpoint_path)\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "ensemble_model = train_model_with_early_stopping(\n",
    "    ensemble_model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"new_model.pth\", resume=True\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "ensemble_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = ensemble_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca9e71b-c499-448d-9c0b-19e021c79ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded successfully.\n",
      "Model state saved separately to 'C:\\Users\\91970\\Desktop\\TruthLens\\New_dataset\\ensemble_model_state.pth'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Define the ensemble model architecture\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Load the individual pre-trained base models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "# Update the output layers to Identity\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "# Send models to device\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Initialize the ensemble model\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = r\"C:\\Users\\91970\\Desktop\\TruthLens\\New_dataset\\new_model.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load model state from checkpoint\n",
    "ensemble_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Checkpoint loaded successfully.\")\n",
    "\n",
    "# Save the model state dictionary separately\n",
    "model_save_path = r\"C:\\Users\\91970\\Desktop\\TruthLens\\New_dataset\\ensemble_model_state.pth\"\n",
    "torch.save(ensemble_model.state_dict(), model_save_path)\n",
    "print(f\"Model state saved separately to '{model_save_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77e1c00-1d60-41e2-b9b5-17aa65e8bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Resumed from epoch 16.\n",
      "Epoch 17/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [44:32<00:00,  6.67s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0287 Acc: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:30<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1180 Acc: 0.9310\n",
      "Checkpoint saved at epoch 17.\n",
      "Epoch 18/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [45:50<00:00,  6.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0268 Acc: 0.9485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:26<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1310 Acc: 0.9275\n",
      "Epoch 19/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [45:50<00:00,  6.86s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0266 Acc: 0.9488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:46<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1092 Acc: 0.9320\n",
      "Checkpoint saved at epoch 19.\n",
      "Epoch 20/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 401/401 [44:08<00:00,  6.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0245 Acc: 0.9492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:28<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1365 Acc: 0.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 63/63 [02:31<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9398\n",
      "Recall: 0.9710\n",
      "F1 Score: 0.9551\n",
      "ROC AUC: 0.9534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Define a custom dataset class to include face detection and cropping\n",
    "class FaceCroppingDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(FaceCroppingDataset, self).__init__(root, transform)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "\n",
    "        # Convert BGR (OpenCV) to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = detector(image, 1)\n",
    "        if len(faces) > 0:\n",
    "            # Crop the first detected face\n",
    "            x, y, w, h = (faces[0].left(), faces[0].top(), faces[0].width(), faces[0].height())\n",
    "            image = image[y:y+h, x:x+w]\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        # Convert image to PIL format and apply transformations\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Data Preparation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Train', transform=data_transforms['train'])\n",
    "val_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Validation', transform=data_transforms['val'])\n",
    "test_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens/New_dataset/Test', transform=data_transforms['test'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[0] is not None and b[1] is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load pre-trained models\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Unfreeze the top layers for fine-tuning\n",
    "for model in [resnet, efficientnet, mobilenet]:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name or \"blocks\" in name or \"features\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Define Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ensemble_model.parameters()), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Save checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, best_acc, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "# Train function with early stopping\n",
    "def train_model_with_early_stopping(\n",
    "    model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"best_model.pth\", resume=False\n",
    "):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_loss = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resumed from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch...\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = running_loss / len(val_dataset)\n",
    "        val_acc = running_corrects.double() / len(val_dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_checkpoint(epoch, model, optimizer, val_acc, checkpoint_path)\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "ensemble_model = train_model_with_early_stopping(\n",
    "    ensemble_model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"new_model.pth\", resume=True\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "ensemble_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = ensemble_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23467bfb-c202-4f85-b776-04ec9de2c7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded successfully.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File C:\\Users\\91970\\Desktop\\TruthLens_Deepfake_Detector cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Save the model state dictionary separately\u001b[39;00m\n\u001b[0;32m     50\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m91970\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTruthLens_Deepfake_Detector\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel state saved separately to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py-gpu\\Lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File C:\\Users\\91970\\Desktop\\TruthLens_Deepfake_Detector cannot be opened."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Define the ensemble model architecture\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Load the individual pre-trained base models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "# Update the output layers to Identity\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "# Send models to device\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Initialize the ensemble model\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = r\"C:\\Users\\91970\\Desktop\\TruthLens_Deepfake_Detector\\new_model.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load model state from checkpoint\n",
    "ensemble_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Checkpoint loaded successfully.\")\n",
    "\n",
    "# Save the model state dictionary separately\n",
    "model_save_path = r\"C:\\Users\\91970\\Desktop\\TruthLens_Deepfake_Detector\"\n",
    "torch.save(ensemble_model.state_dict(), model_save_path)\n",
    "print(f\"Model state saved separately to '{model_save_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e10ab8-4b25-44ce-a430-163c3d7d4a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Resumed from epoch 19.\n",
      "Epoch 20/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 402/402 [41:28<00:00,  6.19s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0295 Acc: 0.9494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [02:18<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1453 Acc: 0.9290\n",
      "Checkpoint saved at epoch 20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 63/63 [02:13<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9489\n",
      "Recall: 0.9813\n",
      "F1 Score: 0.9648\n",
      "ROC AUC: 0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Define a custom dataset class to include face detection and cropping\n",
    "class FaceCroppingDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(FaceCroppingDataset, self).__init__(root, transform)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "\n",
    "        # Convert BGR (OpenCV) to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = detector(image, 1)\n",
    "        if len(faces) > 0:\n",
    "            # Crop the first detected face\n",
    "            x, y, w, h = (faces[0].left(), faces[0].top(), faces[0].width(), faces[0].height())\n",
    "            image = image[y:y+h, x:x+w]\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        # Convert image to PIL format and apply transformations\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Data Preparation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = FaceCroppingDataset(\"C:/Users/91970/Desktop/TruthLens_Deepfake_Detector/New_dataset/Train\", transform=data_transforms['train'])\n",
    "val_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens_Deepfake_Detector/New_dataset/Validation', transform=data_transforms['val'])\n",
    "test_dataset = FaceCroppingDataset('C:/Users/91970/Desktop/TruthLens_Deepfake_Detector/New_dataset/Test', transform=data_transforms['test'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[0] is not None and b[1] is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load pre-trained models\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Unfreeze the top layers for fine-tuning\n",
    "for model in [resnet, efficientnet, mobilenet]:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name or \"blocks\" in name or \"features\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Define Ensemble Model\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ensemble_model.parameters()), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Save checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, best_acc, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "# Train function with early stopping\n",
    "def train_model_with_early_stopping(\n",
    "    model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"best_model.pth\", resume=False\n",
    "):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_loss = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resumed from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch...\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = running_loss / len(val_dataset)\n",
    "        val_acc = running_corrects.double() / len(val_dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_checkpoint(epoch, model, optimizer, val_acc, checkpoint_path)\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model with early stopping and learning rate scheduler\n",
    "ensemble_model = train_model_with_early_stopping(\n",
    "    ensemble_model, criterion, optimizer, scheduler, num_epochs=20, patience=3, checkpoint_path=\"C:\\\\Users\\\\91970\\\\Desktop\\\\TruthLens_Deepfake_Detector\\\\new_model.pth\", resume=True\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "ensemble_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = ensemble_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa83196-77db-4cca-8f28-3893712f1dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded successfully.\n",
      "Model state saved separately to 'C:\\Users\\91970\\Desktop\\TruthLens_Deepfake_Detector\\ensemble_model_state.pth'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Define the ensemble model architecture\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, resnet, efficientnet, mobilenet):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.efficientnet = efficientnet\n",
    "        self.mobilenet = mobilenet\n",
    "        self.fc = nn.Linear(2048 + 1280 + 1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        efficientnet_features = self.efficientnet(x)\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features, mobilenet_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Load the individual pre-trained base models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "# Update the output layers to Identity\n",
    "resnet.fc = nn.Identity()\n",
    "efficientnet.classifier[1] = nn.Identity()\n",
    "mobilenet.classifier[1] = nn.Identity()\n",
    "\n",
    "# Send models to device\n",
    "resnet = resnet.to(device)\n",
    "efficientnet = efficientnet.to(device)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Initialize the ensemble model\n",
    "ensemble_model = EnsembleModel(resnet, efficientnet, mobilenet).to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = r\"C:\\Users\\91970\\Desktop\\TruthLens_Deepfake_Detector\\new_model.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load model state from checkpoint\n",
    "ensemble_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Checkpoint loaded successfully.\")\n",
    "\n",
    "# Specify the save path\n",
    "model_save_path = r\"C:\\Users\\91970\\Desktop\\TruthLens_Deepfake_Detector\\ensemble_model_state.pth\"\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(ensemble_model.state_dict(), model_save_path)\n",
    "print(f\"Model state saved separately to '{model_save_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c4e97-e473-44db-aef7-d86378e2b155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
